import sys

sys.path.append(".")
from astropy.io import ascii
from simple import REFERENCE_TABLES
from astrodb_utils import load_astrodb

from astrodb_utils.sources import (
    find_source_in_db,
    ingest_source,
    logger,
    AstroDBError,
)

from astrodb_utils.publications import (
    find_publication,
    ingest_publication
)


SAVE_DB = False  # save the data files in addition to modifying the .db file
RECREATE_DB = True  # recreates the .db file from the data files
SCHEMA_PATH = "simple/schema.yaml"
db = load_astrodb(
    "SIMPLE.sqlite",
    recreatedb=RECREATE_DB,
    reference_tables=REFERENCE_TABLES,
    felis_schema=SCHEMA_PATH,
)

link = (
    "scripts/ingests/calamari/calamari_data.csv"
)
link_2 = (
    "scripts/ingests/calamari/calamari_refs.csv"
)
calamari_table = ascii.read(
    link,
    format="csv",
    data_start=1,
    header_start=0,
    guess=False,
    fast_reader=False, 
    delimiter=",",
)

ref_table = ascii.read(
    link_2,
    format="csv",
    data_start=0,
    header_start=0,
    guess=False,
    fast_reader=False, 
    delimiter=",",
)

sources_ingested = 0
ref_ingested = 0
companions_ingested = 0
ref_already_exists = 0

def getRef(ref_index):
    ref = ref_index.split(',')[0]
    ref_link = ref_table[int(ref)]['ADS']
    return ref_link

def extractADS(link):
    start = link.find('abs/')+4
    end = link.find('/abstract')
    ads = link[start:end]
    ads = ads.replace("%26", "&")
    return ads


def extractDOI(link):
    start = link.find('article/')+8
    doi = link[start]
    return doi

#ingest publication: Curr20
ingest_publication(
    db=db,
    doi = "10.3847/2041-8213/abc631"
)
ref_ingested+=1

#ingest publication:Fahe21
ingest_publication(
    db=db,
    doi = "10.3847/1538-4357/ac2499",
    bibcode="2022BAAS...54e.224F"
)
ref_ingested+=1
for row in calamari_table:
    #ingest publications
    pub = getRef(row['Ref'])
    #if link provides ADS
    if 'harvard.edu' in pub:
        ref = extractADS(pub)
        print(ref)
        pub_found = find_publication(
            db = db,
            bibcode = ref
        )
        if pub_found[0] == False:
            print(row['Object'])
            print(ref)
            ingest_publication(
                db=db,
                bibcode= ref
            )
            ref_ingested+=1
            print("publications ingested "+str(ref_ingested))
        else:
            print("ref already exists")
            ref_already_exists+=1
    #if link provides doi
    if 'iopscience' in pub:
        ref = extractDOI(pub)
        pub_found = find_publication(
            db=db,
            doi=ref,
        )
        if pub_found[0] == False:
            ingest_publication(
                db=db,
                doi= ref
            )
            ref_ingested+=1
            print("publications ingested " +ref_ingested)
    if pub == None:
        break

"""
    #ingest the sources
    # # read in row
    Dec = row['Dec']
    RA = row['RA']
    object = str(row['Object'])
    primary = str(row['Primary'])

    # check the object in the row is in the DB
    obj_result = find_source_in_db(db=db, source = object, ra=RA, dec=Dec, ra_col_name="ra", dec_col_name="dec")
    if len(obj_result)==0:
        ref = getRef(row['Ref'])
        ingest_source(
            db=db,
            source = object,
            reference = ref,
            ra = RA,
            dec = Dec,
            ra_col_name="ra",
            dec_col_name="dec"
        )

    # check the primary in the row is in the DB
    primary_result = find_source_in_db(db=db, source = primary, ra=RA, dec=Dec, ra_col_name="ra", dec_col_name="dec")
    if len(primary_result)==0:
        print(primary, primary_result)
        ref = getRef(row['Ref'])
        ingest_source(
            db=db,
            source = primary,
            reference = ref,
            ra = RA,
            dec = Dec,
            ra_col_name="ra",
            dec_col_name="dec"
        )"""

    
# WRITE THE JSON FILES
if SAVE_DB:
    db.save_database(directory="data/")