from astrodb_utils import (
    load_astrodb,
    find_source_in_db,
    AstroDBError,
    ingest_names,
    ingest_source,
    ingest_publication,
    find_publication,
)
from astrodb_utils.photometry import ingest_photometry
import sys





sys.path.append(".")
import logging
from astropy.io import ascii
from simple.schema import REFERENCE_TABLES
from math import isnan
import sqlalchemy.exc
from simple.utils.astrometry import ingest_parallax



logger = logging.getLogger(__name__)

# Logger setup
# This will stream all logger messages to the standard output and
# apply formatting for that
logger.propagate = False  # prevents duplicated logging messages
LOGFORMAT = logging.Formatter(
    "%(asctime)s %(levelname)s: %(message)s", datefmt="%m/%d/%Y %I:%M:%S%p"
)
ch = logging.StreamHandler(stream=sys.stdout)
ch.setFormatter(LOGFORMAT)
# To prevent duplicate handlers, only add if they haven't been set previously
if len(logger.handlers) == 0:
    logger.addHandler(ch)
logger.setLevel(logging.INFO)

DB_SAVE = False
RECREATE_DB = True
db = load_astrodb(
    "SIMPLE.sqlite", recreatedb=RECREATE_DB, reference_tables=REFERENCE_TABLES
)

# Load photometry sheet
link = (
    "scripts/ingests/bones_archive/bones_archive_photometry_ads.csv"
)

# read the csv data into an astropy table
bones_sheet_table = ascii.read(
    link,
    format="csv",
    data_start=1,
    header_start=0,
    guess=False,
    fast_reader=False,
    delimiter=",",
)

ingested = 0
already_exists = 0
skipped= 0

#helper method for extracting ads key from link
def extractADS(link):
    start = link.find('abs/')+4
    end = link.find('/abstract')
    ads = link[start:end]
    return ads


for source in bones_sheet_table:
    bones_name = source["NAME"]
    match = None

    ##tries to find match of name in database
    if len(bones_name) > 0 and bones_name != "null":
        match = find_source_in_db(
            db,
            source["NAME"],
            ra=source["RA"],
            dec=source["DEC"],

        )

        if (match == None):
            match = find_source_in_db(
                db,
                source["NAME"],
                ra=source["RA"],
                dec=source["DEC"],
            )
                
        if len(match) == 0:
            #ingest_publications for ads
            ads = extractADS(source["ADS_Link"])
            ads_match = None
            ref = source["Discovery Ref."]
            ads_match = find_publication(db = db, bibcode = ads)

            #if ads not in db, ingest
            if ads_match[0] == False:
                ingest_publication(db = db, bibcode = ads, reference = ref)
            
            try:
                #change this to access ads instead of discovery ref?
                #wrong...............
                reference = find_publication(db = db, bibcode = ads)
                ra = source["RA"]
                dec = source["DEC"]
                ingest_source(
                    db,
                    source = source["NAME"],
                    reference = reference[1],
                    ra = ra,
                    dec = dec,
                    raise_error = True,
                    search_db = True,
                )  # ingest new sources
                ingested += 1
            except AstroDBError as e:
                #None only error is if there is a preexisting source anyways.
                msg = "ingest failed with error: " + str(e)
                logger.warning(msg)
                if "Already in database" in str(e):
                    already_exists+=1
                    continue
                else:
                    raise AstroDBError(msg) from e
        
        elif len(match) == 1:
            already_exists+=1
            skipped+=1
        else:
            skipped+=1
            a = AstroDBError
            logger.warning("ingest failed with error: " + str(a))
            raise AstroDBError(msg) from a
total = ingested+ skipped
logger.info(f"ingested:{ingested}")  #  ingested
logger.info(f"already exists:{already_exists}")  #  due to preexisting data
logger.info(f"skipped:{skipped}")  # 
logger.info(f"total:{total}")
if DB_SAVE:
    db.save_database(directory="data/")
